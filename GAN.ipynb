{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnImEuC2aEYXTR+7vZpnn/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e856fcfd6a7144ad993d4578fb685a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4ac9d543acfa4baa9a620106305cdfc8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0982853c63fb40eda614f515c62fb3b4",
              "IPY_MODEL_51a9a4db2d824f6ab08281af0bf7bc07"
            ]
          }
        },
        "4ac9d543acfa4baa9a620106305cdfc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0982853c63fb40eda614f515c62fb3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0c451ff3ca2e4913a331faccc33ab434",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78014e78d2ee4684a1472962e5a8eb54"
          }
        },
        "51a9a4db2d824f6ab08281af0bf7bc07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2cab33294ec643e0bc259b7a9957c536",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:06, 27735900.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7375acc1d5ea48ba839213450b609716"
          }
        },
        "0c451ff3ca2e4913a331faccc33ab434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78014e78d2ee4684a1472962e5a8eb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2cab33294ec643e0bc259b7a9957c536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7375acc1d5ea48ba839213450b609716": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyTLWrk07joO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdHdr0FahyMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensionality of the latent space\n",
        "DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhZYeIfCBoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 64 # We set the size of the generated images (64x64).\n",
        "batch_size = 64\n",
        "num_of_channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI2lfL-o_lSE",
        "colab_type": "code",
        "outputId": "75ea8635-7831-42a0-d772-f70bc26ab5dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu == True:\n",
        "  print(\"CUDA is available!  Training on GPU ...\")\n",
        "else:\n",
        "  print(\"CUDA is not available.  Training on CPU ...\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Ly3kO4AeAx",
        "colab_type": "code",
        "outputId": "5a870fab-7641-48fa-e4b2-6dd69e2f3e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.Scale(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOI9V-vVCHMY",
        "colab_type": "code",
        "outputId": "8f4dadc1-f98f-4afd-cbdf-40aa9e040bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "e856fcfd6a7144ad993d4578fb685a48",
            "4ac9d543acfa4baa9a620106305cdfc8",
            "0982853c63fb40eda614f515c62fb3b4",
            "51a9a4db2d824f6ab08281af0bf7bc07",
            "0c451ff3ca2e4913a331faccc33ab434",
            "78014e78d2ee4684a1472962e5a8eb54",
            "2cab33294ec643e0bc259b7a9957c536",
            "7375acc1d5ea48ba839213450b609716"
          ]
        }
      },
      "source": [
        "dataset = datasets.CIFAR10(\n",
        "    'data',\n",
        "    download = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e856fcfd6a7144ad993d4578fb685a48",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdxTd73TOWA",
        "colab_type": "code",
        "outputId": "ff1c8f76-daae-4fe8-ad18-2e6a56dd430b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnIHqDSeUlKj",
        "colab_type": "code",
        "outputId": "250ad886-9b6f-4653-ed76-b28669b9b658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Scale(size=64, interpolation=PIL.Image.BILINEAR)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjyRR3Z4WIyj",
        "colab_type": "code",
        "outputId": "932955ac-3182-4048-dd8b-91cc463fc2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.cifar.CIFAR10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llW8JyB1UsDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW_8uNRnZkwn",
        "colab_type": "code",
        "outputId": "49ba1987-2264-436d-d08a-9611b493d3cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "class make_generator_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(make_generator_model, self).__init__()\n",
        "    \n",
        "    self.main = nn.Sequential(\n",
        "        \n",
        "        nn.ConvTranspose2d(DIM, 512, kernel_size=4, stride=1, padding=0, bias=False), # inverse convolution since we are turning a vector into an image\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(64, num_of_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "    return self.main(noise)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "print(generator)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_generator_model(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czbrSJAkgLNr",
        "colab_type": "code",
        "outputId": "116eda86-8241-40dd-b7c2-cc8f9a8c4874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "class make_discriminator_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(make_discriminator_model, self).__init__()\n",
        "\n",
        "    self.main = nn.Sequential(\n",
        "        \n",
        "        # Below conv layer gets torch.Size([64, 3, 64, 64])\n",
        "        nn.Conv2d(num_of_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 64, 32, 32])\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 128, 16, 16])\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 256, 8, 8])\n",
        "        nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 512, 4, 4]) & outputs torch.Size([64, 1, 1, 1])\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, image):                         \n",
        "    # print(image.shape)                 # torch.Size([64, 3, 64, 64])\n",
        "    output = self.main(image)\n",
        "    # print(output.shape)                # torch.Size([64, 1, 1, 1]) is the final output after going through sequential steps above          \n",
        "    return output.view(-1)               # the .view converts it from torch.Size([64, 1, 1, 1]) to torch.Size([64])\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "print(discriminator)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_discriminator_model(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEho0UPoBtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gcVljogR5A4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_generator = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6URSgz9ZtoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_d(model, loss, optimizer, inputs, labels):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  inputs = Variable(inputs, requires_grad=False)   # To create torch variable. By default requires_grad is False\n",
        "  labels = Variable(labels, requires_grad=False)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # get output from the model, given the inputs\n",
        "  logps = model.forward(inputs)                    # torch.Size([64]). Same shape as labels (ones & zeros)\n",
        "\n",
        "  # get loss for the predicted output\n",
        "  cost = loss.forward(logps, labels)\n",
        "\n",
        "  # get gradients w.r.t to parameters\n",
        "  cost.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  return cost.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmrxiUXVZN64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_g(model, loss, optimizer, inputs, labels):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  inputs = Variable(inputs, requires_grad=False)   # To create torch variable. By default requires_grad is False\n",
        "  labels = Variable(labels, requires_grad=False)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # get output from the model, given the inputs\n",
        "  logps = discriminator.forward(inputs)                    # torch.Size([64]). Same shape as labels (ones & zeros)\n",
        "\n",
        "  # get loss for the predicted output\n",
        "  cost = loss.forward(logps, labels)\n",
        "\n",
        "  # get gradients w.r.t to parameters\n",
        "  cost.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  return cost.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPb6Jlg1M400",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_generated_images(epoch):\n",
        "  noise = torch.randn(batch_size, DIM, 1, 1)  # 100 feature maps of size 1x1, torch.Size([64, 100, 1, 1])\n",
        "  noise = Variable(noise, requires_grad=False)                   # torch.Size([64, 100, 1, 1])\n",
        "  images = generator.forward(noise)                         # torch.Size([64, 3, 64, 64])\n",
        "  #noise = np.random.randn(25, DIM)                             # (25, 100)\n",
        "  #generated_images = generator.predict(noise)                  # (25, 784)\n",
        "  #generated_images = generated_images.reshape(25,28,28)        # (25, 28, 28)\n",
        "  fig, axes = plt.subplots(8, 8, figsize=(20,20))              # We use 5 rows & 5 columns as there are 25 images\n",
        "  axes = axes.flatten()\n",
        "  for img, ax in zip(images, axes):\n",
        "    img = img / 2 + 0.5 \n",
        "    img=img.permute(1,2,0)\n",
        "    #ax.imshow(img)\n",
        "    #plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('gan_generated_image %d.png' %epoch)\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smkOuaYrUXb5",
        "colab_type": "code",
        "outputId": "9edc3450-3fcf-4f0a-a81e-b6b4df944be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for batch_idx, (real_images, labels) in enumerate(dataloader):   \n",
        "  # we don't care about labels & create our own labels\n",
        "  # batch_idx goes from 0 to (50,000 / batch_size) = 781.25 = truncated to 781\n",
        "\n",
        "    ############################\n",
        "    # TRAINING THE DISCRIMINATOR\n",
        "    ############################\n",
        "    # print(real_images.shape)                                     # torch.Size([64, 3, 64, 64])\n",
        "    ones = torch.ones(batch_size)                                  # torch.Size([64])\n",
        "    disc_loss_real = train_d(discriminator, criterion, optimizer_discriminator, real_images, ones)\n",
        "    # print(disc_loss_real)\n",
        "\n",
        "    noise = torch.randn(batch_size, DIM, 1, 1)  # 100 feature maps of size 1x1, torch.Size([64, 100, 1, 1])\n",
        "    noise = Variable(noise, requires_grad=False)                   # torch.Size([64, 100, 1, 1])\n",
        "    fake_images = generator.forward(noise)                         # torch.Size([64, 3, 64, 64])\n",
        "\n",
        "    zeros = torch.zeros(batch_size)                                # torch.Size([64])\n",
        "    disc_loss_fake = train_d(discriminator, criterion, optimizer_discriminator, fake_images, zeros)\n",
        "    # print(disc_loss_fake)\n",
        "    discriminator_loss = (disc_loss_real + disc_loss_fake) / 2\n",
        "\n",
        "    ########################\n",
        "    # TRAINING THE GENERATOR\n",
        "    ########################\n",
        "    gen_loss = train_g(generator, criterion, optimizer_generator, fake_images, ones)\n",
        "\n",
        "    if batch_idx % 500 == 0:\n",
        "      print(f\"epoch: {epoch}/{epochs}, discriminator_loss: {discriminator_loss:.2f}, generator_loss: {gen_loss:.2f}\")\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      plot_generated_images(batch_idx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0/4, discriminator_loss: 1.56, generator_loss: 1.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FcBmdxVR67H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "image = mpimg.imread('gan_generated_image 0.png')\n",
        "plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_2SiuYsR76Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "image = mpimg.imread('gan_generated_image 2.png')\n",
        "plt.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}