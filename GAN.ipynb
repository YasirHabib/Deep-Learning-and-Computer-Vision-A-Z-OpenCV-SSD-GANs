{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzpSmNNNX6ybUqic8sFuEF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyTLWrk07joO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdHdr0FahyMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensionality of the latent space\n",
        "DIM = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhZYeIfCBoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 64 # We set the size of the generated images (64x64).\n",
        "batch_size = 64\n",
        "num_of_channels = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI2lfL-o_lSE",
        "colab_type": "code",
        "outputId": "ddf39868-9bc2-4d3a-89e6-4bcd55cdc8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu == True:\n",
        "  print(\"CUDA is available!  Training on GPU ...\")\n",
        "else:\n",
        "  print(\"CUDA is not available.  Training on CPU ...\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Ly3kO4AeAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cef26d2b-d858-417e-fb8f-e4f9d0fa538c"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.Scale(image_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOI9V-vVCHMY",
        "colab_type": "code",
        "outputId": "15d1754b-d10f-44e9-8aca-909cbf0f70e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = datasets.CIFAR10(\n",
        "    'data',\n",
        "    download = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHdxTd73TOWA",
        "colab_type": "code",
        "outputId": "511794d2-6556-47a6-821e-d56883b40ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnIHqDSeUlKj",
        "colab_type": "code",
        "outputId": "b042f467-feb4-4ead-adac-c48cc449a3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "dataset"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Scale(size=64, interpolation=PIL.Image.BILINEAR)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjyRR3Z4WIyj",
        "colab_type": "code",
        "outputId": "3701f80f-4c83-4951-a0d8-e9f83b85af38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.cifar.CIFAR10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llW8JyB1UsDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW_8uNRnZkwn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ee4eef34-0ed4-450c-ed0e-906feef65d80"
      },
      "source": [
        "class make_generator_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(make_generator_model, self).__init__()\n",
        "    \n",
        "    self.main = nn.Sequential(\n",
        "        \n",
        "        nn.ConvTranspose2d(DIM, 512, kernel_size=4, stride=1, padding=0, bias=False), # inverse convolution since we are turning a vector into an image\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        nn.ConvTranspose2d(64, num_of_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "    return self.main(noise)\n",
        "\n",
        "generator = make_generator_model()\n",
        "\n",
        "print(generator)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_generator_model(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czbrSJAkgLNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "44af94e2-dd0a-41e0-a87c-d8276ceb2477"
      },
      "source": [
        "class make_discriminator_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(make_discriminator_model, self).__init__()\n",
        "\n",
        "    self.main = nn.Sequential(\n",
        "        \n",
        "        # Below conv layer gets torch.Size([64, 3, 64, 64])\n",
        "        nn.Conv2d(num_of_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 64, 32, 32])\n",
        "        nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 128, 16, 16])\n",
        "        nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 256, 8, 8])\n",
        "        nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # Below conv layer gets torch.Size([64, 512, 4, 4]) & outputs torch.Size([64, 1, 1, 1])\n",
        "        nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, image):                         \n",
        "    # print(image.shape)                 # torch.Size([64, 3, 64, 64])\n",
        "    output = self.main(image)\n",
        "    # print(output.shape)                # torch.Size([64, 1, 1, 1]) is the final output after going through sequential steps above          \n",
        "    return output.view(-1)               # the .view converts it from torch.Size([64, 1, 1, 1]) to torch.Size([64])\n",
        "\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "print(discriminator)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_discriminator_model(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEho0UPoBtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gcVljogR5A4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_generator = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6URSgz9ZtoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_discriminator(model, loss, optimizer, inputs, labels):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  inputs = Variable(inputs, requires_grad=False)   # To create torch variable\n",
        "                                                   # By default requires_grad is False\n",
        "  labels = Variable(labels, requires_grad=False)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # get output from the model, given the inputs\n",
        "  logps = model.forward(inputs)                    # torch.Size([64]). Same shape as labels (ones & zeros)\n",
        "\n",
        "  # get loss for the predicted output\n",
        "  cost = loss.forward(logps, labels)\n",
        "\n",
        "  # get gradients w.r.t to parameters\n",
        "  cost.backward()\n",
        "\n",
        "  # update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  return cost.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smkOuaYrUXb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for batch_idx, (real_images, labels) in enumerate(dataloader):   # we don't care about labels & create our own labels\n",
        "    # print(real_images.shape)                                     # torch.Size([64, 3, 64, 64])\n",
        "    ones = torch.ones(batch_size)                                  # torch.Size([64])\n",
        "    disc_loss_real = train_discriminator(discriminator, criterion, optimizer_discriminator, real_images, ones)\n",
        "\n",
        "    noise = torch.randn(batch_size, DIM, 1, 1)  # 100 feature maps of size 1x1, torch.Size([64, 100, 1, 1])\n",
        "    noise = Variable(noise, requires_grad=False)                   # torch.Size([64, 100, 1, 1])\n",
        "    fake_images = generator.forward(noise)                         # torch.Size([64, 3, 64, 64])\n",
        "\n",
        "    zeros = torch.zeros(batch_size)                                # torch.Size([64])\n",
        "    disc_loss_fake = train_discriminator(discriminator, criterion, optimizer_discriminator, fake_images, zeros)\n",
        "\n",
        "    discriminator_loss = (disc_loss_real + disc_loss_fake) / 2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}